{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>TEXT to tensors</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Tokenisation</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BEGIN_SYMBOL = '^'\n",
    "END_SYMBOL = '$'\n",
    "BLANK = ' '\n",
    "CHAR_SET = set(string.ascii_lowercase + BEGIN_SYMBOL + END_SYMBOL + BLANK)\n",
    "CHAR_NUM = len(CHAR_SET)\n",
    "CHAR_TO_INDICES = {c:i for i, c in enumerate(CHAR_SET)}\n",
    "INDICES_TO_CHAR = {i:c for c, i in CHAR_TO_INDICES.iteritems()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 0,\n",
       " '$': 1,\n",
       " '^': 2,\n",
       " 'a': 3,\n",
       " 'b': 5,\n",
       " 'c': 4,\n",
       " 'd': 7,\n",
       " 'e': 6,\n",
       " 'f': 9,\n",
       " 'g': 8,\n",
       " 'h': 11,\n",
       " 'i': 10,\n",
       " 'j': 13,\n",
       " 'k': 12,\n",
       " 'l': 15,\n",
       " 'm': 14,\n",
       " 'n': 17,\n",
       " 'o': 16,\n",
       " 'p': 19,\n",
       " 'q': 18,\n",
       " 'r': 21,\n",
       " 's': 20,\n",
       " 't': 23,\n",
       " 'u': 22,\n",
       " 'v': 25,\n",
       " 'w': 24,\n",
       " 'x': 27,\n",
       " 'y': 26,\n",
       " 'z': 28}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CHAR_TO_INDICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: ' ',\n",
       " 1: '$',\n",
       " 2: '^',\n",
       " 3: 'a',\n",
       " 4: 'c',\n",
       " 5: 'b',\n",
       " 6: 'e',\n",
       " 7: 'd',\n",
       " 8: 'g',\n",
       " 9: 'f',\n",
       " 10: 'i',\n",
       " 11: 'h',\n",
       " 12: 'k',\n",
       " 13: 'j',\n",
       " 14: 'm',\n",
       " 15: 'l',\n",
       " 16: 'o',\n",
       " 17: 'n',\n",
       " 18: 'q',\n",
       " 19: 'p',\n",
       " 20: 's',\n",
       " 21: 'r',\n",
       " 22: 'u',\n",
       " 23: 't',\n",
       " 24: 'w',\n",
       " 25: 'v',\n",
       " 26: 'y',\n",
       " 27: 'x',\n",
       " 28: 'z'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INDICES_TO_CHAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize(word, seq_len, vec_size):\n",
    "    vec = np.zeros((seq_len, vec_size), dtype=int)\n",
    "    for i, ch in enumerate(word):\n",
    "        vec[i, CHAR_TO_INDICES[ch]] = 1\n",
    "\n",
    "    for i in range(len(word), seq_len):\n",
    "        vec[i, CHAR_TO_INDICES[END_SYMBOL]] = 1\n",
    "\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 150, 29)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "say = 'ewqwr qerqewr qrwrqwr qwrwqrqw rqwrqw rqwrqwr qwrwqrq wqrwqr qwrqwrqw'\n",
    "X = np.zeros((1,150, 29), dtype=int)\n",
    "Word = BEGIN_SYMBOL + say.lower().strip() + END_SYMBOL\n",
    "X[0] = vectorize(Word, 150, 29)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0][6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ewqwr qerqewr qrwrqwr qwrwqrqw rqwrqw rqwrqwr qwrwqrq wqrwqr qwrqwrqw\n"
     ]
    }
   ],
   "source": [
    "print ''.join([INDICES_TO_CHAR[i]\n",
    "               for i in X[0].argmax(axis=1)\n",
    "               if INDICES_TO_CHAR[i] not in (BEGIN_SYMBOL,END_SYMBOL)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>CountVectorizer</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(min_df=1)\n",
    "vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = [\n",
    "     'This is the first document.',\n",
    "     'This is the second second document.',\n",
    "     'And the third one.',\n",
    "     'Is this the first document?',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x9 sparse matrix of type '<type 'numpy.int64'>'\n",
       "\twith 19 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = vectorizer.fit_transform(corpus)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'and',\n",
       " u'document',\n",
       " u'first',\n",
       " u'is',\n",
       " u'one',\n",
       " u'second',\n",
       " u'the',\n",
       " u'third',\n",
       " u'this']"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 0, 1, 0, 1],\n",
       "       [0, 1, 0, 1, 0, 2, 1, 0, 1],\n",
       "       [1, 0, 0, 0, 1, 0, 1, 1, 0],\n",
       "       [0, 1, 1, 1, 0, 0, 1, 0, 1]])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>tf - idf</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from textblob import TextBlob as tb\n",
    "\n",
    "def tf(word, blob):\n",
    "    return (float)(blob.words.count(word)) / (float)(len(blob.words))  # frequency of word in sentence\n",
    "\n",
    "def n_containing(word, bloblist):\n",
    "    return (float)(sum(1 for blob in bloblist if word in blob)) # no. of word in corpus\n",
    "\n",
    "def idf(word, bloblist):\n",
    "    return (float)(math.log(len(bloblist)) / (float)(1 + n_containing(word, bloblist))) # idf \n",
    "\n",
    "def tfidf(word, blob, bloblist):\n",
    "    return (float)((float)(tf(word, blob)) * (float)(idf(word, bloblist))) # tf * idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "document1 = tb(\"\"\"Python is a 2000 made-for-TV horror movie directed by Richard\n",
    "Clabaugh. The film features several cult favorite actors, including William\n",
    "Zabka of The Karate Kid fame, Wil Wheaton, Casper Van Dien, Jenny McCarthy,\n",
    "Keith Coogan, Robert Englund (best known for his role as Freddy Krueger in the\n",
    "A Nightmare on Elm Street series of films), Dana Barron, David Bowe, and Sean\n",
    "Whalen. The film concerns a genetically engineered snake, a python, that\n",
    "escapes and unleashes itself on a small town. It includes the classic final\n",
    "girl scenario evident in films like Friday the 13th. It was filmed in Los Angeles,\n",
    " California and Malibu, California. Python was followed by two sequels: Python\n",
    " II (2002) and Boa vs. Python (2004), both also made-for-TV films.\"\"\")\n",
    "\n",
    "document2 = tb(\"\"\"Python, from the Greek word, is a genus of\n",
    "nonvenomous pythons[2] found in Africa and Asia. Currently, 7 species are\n",
    "recognised.[2] A member of this genus, P. reticulatus, is among the longest\n",
    "snakes known.\"\"\")\n",
    "\n",
    "document3 = tb(\"\"\"The Colt Python is a .357 Magnum caliber revolver formerly\n",
    "manufactured by Colt's Manufacturing Company of Hartford, Connecticut.\n",
    "It is sometimes referred to as a \"Combat Magnum\".[1] It was first introduced\n",
    "in 1955, the same year as Smith &amp; Wesson's M29 .44 Magnum. The now discontinued\n",
    "Colt Python targeted the premium revolver market segment. Some firearm\n",
    "collectors and writers such as Jeff Cooper, Ian V. Hogg, Chuck Hawks, Leroy\n",
    "Thompson, Renee Smeets and Martin Dougherty have described the Python as the\n",
    "finest production revolver ever made.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i, blob in enumerate(bloblist):\n",
    "    print(\"Top words in document {}\".format(i + 1))\n",
    "    scores = {word: tfidf(word, blob, bloblist) for word in blob.words}\n",
    "    sorted_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    for word, score in sorted_words[:3]:\n",
    "        print(\"\\tWord: {}, TF-IDF: {}\".format(word, round(score, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc1 = [word for word in document1.words]\n",
    "doc2 = [word for word in document2.words]\n",
    "doc3 = [word for word in document3.words]\n",
    "#bloblist = [doc1, doc2, doc3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bloblist = [document1, document2, document3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bloblist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top words in document 1\n",
      "\tWord: The, TF-IDF: 0.01801\n",
      "\tWord: python, TF-IDF: 0.01501\n",
      "\tWord: A, TF-IDF: 0.01501\n",
      "\tWord: films, TF-IDF: 0.01351\n",
      "\tWord: the, TF-IDF: 0.01351\n",
      "Top words in document 2\n",
      "\tWord: genus, TF-IDF: 0.03052\n",
      "\tWord: A, TF-IDF: 0.02034\n",
      "\tWord: among, TF-IDF: 0.01526\n",
      "\tWord: snakes, TF-IDF: 0.01526\n",
      "\tWord: is, TF-IDF: 0.01526\n",
      "Top words in document 3\n",
      "\tWord: The, TF-IDF: 0.02469\n",
      "\tWord: Magnum, TF-IDF: 0.01852\n",
      "\tWord: revolver, TF-IDF: 0.01852\n",
      "\tWord: Colt, TF-IDF: 0.01852\n",
      "\tWord: the, TF-IDF: 0.01852\n"
     ]
    }
   ],
   "source": [
    "for i, blob in enumerate(bloblist):\n",
    "    print(\"Top words in document {}\".format(i + 1))\n",
    "    scores = {word: tfidf(word, blob, bloblist) for word in blob.words}\n",
    "    sorted_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    for word, score in sorted_words[:5]:\n",
    "        print(\"\\tWord: {}, TF-IDF: {}\".format(word, round(score, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc3_vec = [scores[w] for w in doc3] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.024687916599283367,\n",
       " 0.018515937449462522,\n",
       " 0.009257968724731261,\n",
       " 0.006171979149820842,\n",
       " 0.006171979149820842,\n",
       " 0.006171979149820842,\n",
       " 0.018515937449462522,\n",
       " 0.006171979149820842,\n",
       " 0.018515937449462522,\n",
       " 0.006171979149820842,\n",
       " 0.006171979149820842,\n",
       " 0.0041146527665472275,\n",
       " 0.018515937449462522,\n",
       " 0.012343958299641683,\n",
       " 0.006171979149820842,\n",
       " 0.006171979149820842,\n",
       " 0.003085989574910421,\n",
       " 0.006171979149820842,\n",
       " 0.006171979149820842,\n",
       " 0.008229305533094455,\n",
       " 0.006171979149820842,\n",
       " 0.006171979149820842,\n",
       " 0.006171979149820842,\n",
       " 0.0041146527665472275,\n",
       " 0.01645861106618891,\n",
       " 0.006171979149820842,\n",
       " 0.006171979149820842,\n",
       " 0.018515937449462522,\n",
       " 0.0041146527665472275,\n",
       " 0.008229305533094455,\n",
       " 0.0041146527665472275,\n",
       " 0.006171979149820842,\n",
       " 0.006171979149820842,\n",
       " 0.003085989574910421,\n",
       " 0.006171979149820842,\n",
       " 0.018515937449462522,\n",
       " 0.006171979149820842,\n",
       " 0.006171979149820842,\n",
       " 0.01645861106618891,\n",
       " 0.006171979149820842,\n",
       " 0.006171979149820842,\n",
       " 0.006171979149820842,\n",
       " 0.012343958299641683,\n",
       " 0.006171979149820842,\n",
       " 0.006171979149820842,\n",
       " 0.018515937449462522,\n",
       " 0.024687916599283367,\n",
       " 0.003085989574910421,\n",
       " 0.006171979149820842,\n",
       " 0.018515937449462522,\n",
       " 0.009257968724731261,\n",
       " 0.006171979149820842,\n",
       " 0.018515937449462522,\n",
       " 0.006171979149820842,\n",
       " 0.018515937449462522,\n",
       " 0.006171979149820842,\n",
       " 0.006171979149820842,\n",
       " 0.006171979149820842,\n",
       " 0.006171979149820842,\n",
       " 0.006171979149820842,\n",
       " 0.006171979149820842,\n",
       " 0.006171979149820842,\n",
       " 0.006171979149820842,\n",
       " 0.01645861106618891,\n",
       " 0.006171979149820842,\n",
       " 0.006171979149820842,\n",
       " 0.006171979149820842,\n",
       " 0.0041146527665472275,\n",
       " 0.006171979149820842,\n",
       " 0.006171979149820842,\n",
       " 0.006171979149820842,\n",
       " 0.006171979149820842,\n",
       " 0.006171979149820842,\n",
       " 0.006171979149820842,\n",
       " 0.006171979149820842,\n",
       " 0.006171979149820842,\n",
       " 0.006171979149820842,\n",
       " 0.006171979149820842,\n",
       " 0.006171979149820842,\n",
       " 0.006171979149820842,\n",
       " 0.018515937449462522,\n",
       " 0.009257968724731261,\n",
       " 0.01645861106618891,\n",
       " 0.018515937449462522,\n",
       " 0.006171979149820842,\n",
       " 0.006171979149820842,\n",
       " 0.018515937449462522,\n",
       " 0.0041146527665472275,\n",
       " 0.0041146527665472275]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc3_vec # sentence in vector form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_count = {word:(tf(word, document1)*100) for word in document1.words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'13th': 0.819672131147541,\n",
       " '2000': 0.819672131147541,\n",
       " '2002': 0.819672131147541,\n",
       " '2004': 0.819672131147541,\n",
       " 'A': 4.098360655737705,\n",
       " 'Angeles': 0.819672131147541,\n",
       " 'Barron': 0.819672131147541,\n",
       " 'Boa': 0.819672131147541,\n",
       " 'Bowe': 0.819672131147541,\n",
       " 'California': 1.639344262295082,\n",
       " 'Casper': 0.819672131147541,\n",
       " 'Clabaugh': 0.819672131147541,\n",
       " 'Coogan': 0.819672131147541,\n",
       " 'Dana': 0.819672131147541,\n",
       " 'David': 0.819672131147541,\n",
       " 'Dien': 0.819672131147541,\n",
       " 'Elm': 0.819672131147541,\n",
       " 'Englund': 0.819672131147541,\n",
       " 'Freddy': 0.819672131147541,\n",
       " 'Friday': 0.819672131147541,\n",
       " 'II': 0.819672131147541,\n",
       " 'It': 1.639344262295082,\n",
       " 'Jenny': 0.819672131147541,\n",
       " 'Karate': 0.819672131147541,\n",
       " 'Keith': 0.819672131147541,\n",
       " 'Kid': 0.819672131147541,\n",
       " 'Krueger': 0.819672131147541,\n",
       " 'Los': 0.819672131147541,\n",
       " 'Malibu': 0.819672131147541,\n",
       " 'McCarthy': 0.819672131147541,\n",
       " 'Nightmare': 0.819672131147541,\n",
       " 'Python': 4.098360655737705,\n",
       " 'Richard': 0.819672131147541,\n",
       " 'Robert': 0.819672131147541,\n",
       " 'Sean': 0.819672131147541,\n",
       " 'Street': 0.819672131147541,\n",
       " 'The': 4.918032786885246,\n",
       " 'Van': 0.819672131147541,\n",
       " 'Whalen': 0.819672131147541,\n",
       " 'Wheaton': 0.819672131147541,\n",
       " 'Wil': 0.819672131147541,\n",
       " 'William': 0.819672131147541,\n",
       " 'Zabka': 0.819672131147541,\n",
       " 'a': 4.098360655737705,\n",
       " 'actors': 0.819672131147541,\n",
       " 'also': 0.819672131147541,\n",
       " 'and': 3.278688524590164,\n",
       " 'as': 0.819672131147541,\n",
       " 'best': 0.819672131147541,\n",
       " 'both': 0.819672131147541,\n",
       " 'by': 1.639344262295082,\n",
       " 'classic': 0.819672131147541,\n",
       " 'concerns': 0.819672131147541,\n",
       " 'cult': 0.819672131147541,\n",
       " 'directed': 0.819672131147541,\n",
       " 'engineered': 0.819672131147541,\n",
       " 'escapes': 0.819672131147541,\n",
       " 'evident': 0.819672131147541,\n",
       " 'fame': 0.819672131147541,\n",
       " 'favorite': 0.819672131147541,\n",
       " 'features': 0.819672131147541,\n",
       " 'film': 1.639344262295082,\n",
       " 'filmed': 0.819672131147541,\n",
       " 'films': 2.459016393442623,\n",
       " 'final': 0.819672131147541,\n",
       " 'followed': 0.819672131147541,\n",
       " 'for': 0.819672131147541,\n",
       " 'genetically': 0.819672131147541,\n",
       " 'girl': 0.819672131147541,\n",
       " 'his': 0.819672131147541,\n",
       " 'horror': 0.819672131147541,\n",
       " 'in': 2.459016393442623,\n",
       " 'includes': 0.819672131147541,\n",
       " 'including': 0.819672131147541,\n",
       " 'is': 0.819672131147541,\n",
       " 'itself': 0.819672131147541,\n",
       " 'known': 0.819672131147541,\n",
       " 'like': 0.819672131147541,\n",
       " 'made-for-TV': 1.639344262295082,\n",
       " 'movie': 0.819672131147541,\n",
       " 'of': 1.639344262295082,\n",
       " 'on': 1.639344262295082,\n",
       " 'python': 4.098360655737705,\n",
       " 'role': 0.819672131147541,\n",
       " 'scenario': 0.819672131147541,\n",
       " 'sequels': 0.819672131147541,\n",
       " 'series': 0.819672131147541,\n",
       " 'several': 0.819672131147541,\n",
       " 'small': 0.819672131147541,\n",
       " 'snake': 0.819672131147541,\n",
       " 'that': 0.819672131147541,\n",
       " 'the': 4.918032786885246,\n",
       " 'town': 0.819672131147541,\n",
       " 'two': 0.819672131147541,\n",
       " 'unleashes': 0.819672131147541,\n",
       " 'vs': 0.819672131147541,\n",
       " 'was': 1.639344262295082}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'this': 1.0, u'very': 1.0, u'is': 1.0, u'strange': 1.4054651081081644, u'nice': 1.4054651081081644}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = [\"This is very strange\",\n",
    "          \"This is very nice\"]\n",
    "vectorizer = TfidfVectorizer(min_df=1)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "idf = vectorizer.idf_\n",
    "print dict(zip(vectorizer.get_feature_names(), idf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.44832087,  0.        ,  0.63009934,  0.44832087,  0.44832087],\n",
       "       [ 0.44832087,  0.63009934,  0.        ,  0.44832087,  0.44832087]])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
